---
title: "p8105_hw3_jy2965"
author: "Jun Yin"
date: "10/11/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(patchwork)
library(hexbin)
library(p8105.datasets)
```

## problem 1


```{r}
data("instacart")
instacart = janitor::clean_names(instacart)
head(instacart)
skimr::skim(instacart)
head(instacart, n=1)
```

This "instacart" dataset documents the shoppoing history of different client. From the table above, we know that there are total 1384617 observations and 15 variables. Among these 15 variables, 4 of them are character variables ("aisle", "department", "eval_set", "product_name"), and the others are all numeric variables. 

There are 15 variables in this dataset:

order_id: order identifier
product_id: product identifier
add_to_cart_order: order in which each product was added to cart
reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
user_id: customer identifier
eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
order_number: the order sequence number for this user (1=first, n=nth)
order_dow: the day of the week on which the order was placed
order_hour_of_day: the hour of the day on which the order was placed
days_since_prior_order: days since the last order, capped at 30, NA if order_number=1
product_name: name of the product
aisle_id: aisle identifier
department_id: department identifier
aisle: the name of the aisle
department: the name of the department

For example, the first line in the dataset, user with user_id 112108 placed an order(order_id: 1), buying Bulgarian Yogurt at 10am on Thursday. And the order is this user's fourth order.

## Answer questions
*How many aisles are there, and which aisles are the most items ordered from?*
```{r}
n_distinct(instacart$aisle)

instacart %>% 
  group_by(aisle) %>% 
  summarize(n_aisle = n()) %>% 
  arrange(desc(n_aisle))
```
There are 134 distinct aisles, and "fresh vegetables" aisle is the most items ordered from.

*Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.*
```{r}
#whole list of aisles(it not required but I prefer to keep this plot to compare with ">10000" plot)
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_aisle = n()) %>% 
  arrange(desc(n_aisle)) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle, .desc = FALSE)) %>% 
  ggplot(aes(x = aisle, y = n_aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "the Number of Items Ordered in Each Aisle",
    x = "Aisle",
    y = "Number of Items",
    caption = "Data from p8105.datasets package") 
#list of aisles > 10000
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_aisle = n()) %>% 
  arrange(desc(n_aisle)) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle, .desc = FALSE)) %>% 
  filter(n_aisle>=10000) %>% 
      ggplot(aes(x = aisle, y = n_aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "the Number of Items Ordered in Each Aisle",
    x = "Aisle",
    y = "Number of Items",
    caption = "Data from p8105.datasets package") 
  

```

Form the figure above, we know that the number of items in "fresh vegetables" and "fresh fruits" are much higher then any other asiles

*Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.*

```{r}
instacart %>% 
  group_by(product_name, aisle) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(n_items = n()) %>% 
  group_by(aisle) %>% 
  filter(n_items == max(n_items)) %>% 
  knitr::kable()


```



The most popular item in aisle "baking ingredients" is "Light Brown Sugar", the most popular item in aisle "dog food care" is "Snack Sticks Chicken & Rice Recipe Dog Treats", and the most popular item in aisle "packaged vegetables fruits" is "Organic Baby Spinach".

*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*
```{r}
instacart$order_dow = recode(instacart$order_dow, "0" = "Sunday", "6" = "Saturday", "5" = "Friday", "4" = "Thursday", "3" = "Wednesday", "2" = "Tuesday", "1" = "Monday") %>% 
  factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
instacart %>% 
  group_by(product_name,order_dow) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()

```




People tends to buy Coffee Ice Cream around 1pm to 3pm, but on Friday people may buy the cream a little bit earlier, around 12:00 at noon. And people usually buy Pink Lady Apples around noon from 11am to 2pm.

## problem 2


*format the data to use appropriate variable names*
*focus on the “Overall Health” topic*
*include only responses from “Excellent” to “Poor”*
*organize responses as a factor taking levels ordered from “Poor” to “Excellent”*
```{r}
data("brfss_smart2010")
brfss = janitor::clean_names(brfss_smart2010) %>% 
rename(state = locationabbr, county = locationdesc) %>% 
filter(topic == "Overall Health") %>% 
filter(response %in% c("Excellent", "Very good","Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair","Good", "Very good", "Excellent")))
  
```


*Using this dataset, do or answer the following (commenting on the results of each):*

*In 2002, which states were observed at 7 or more locations? What about in 2010?*
```{r}
#year in 2002
brfss %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  summarise(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>% 
  knitr::kable()
#year in 2010
brfss %>% 
  filter(year == "2010") %>% 
  group_by(state) %>% 
  summarise(n_location = n_distinct(county)) %>% 
  filter(n_location >= 7) %>% 
  knitr::kable()


```



In 2002, there are 6 states being observed at 7 or more location. Here is the list of 6 states: CT, FL, MA, NC, NJ,PA; As for 2010, there are 14 states being observed at 7 or more location.




*Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).*


```{r}
brfss %>% 
  filter(response %in% "Excellent") %>% 
  group_by(year,state) %>% 
  summarise(avg_datavalue = mean(data_value)) %>% 
  select(year, state, avg_datavalue) %>% 
ggplot(aes(x = year, y = avg_datavalue, color = state)) +
  geom_line() +
  labs(
        title = "Spaghetti-Plot average value over time within a state",
        y = "average value",
        x = "Year",
        caption = "Data from p8105.datasets package") +
  theme(legend.position = "bottom",
        legend.key.width = unit(.25, 'cm')) +
  guides(color = guide_legend(ncol = 15))
  

```



The figure above shows the average value over time within a state. We can see the concussion in each state.


*Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.*

```{r}
brfss_plot = brfss %>% 
  filter(year %in% c("2006", "2010"), state == "NY") %>%
  group_by(year,county,response) %>% 
  select(year,county,response,data_value)
  ggplot(brfss_plot,aes(fill= response, y=data_value, x=county)) +
  geom_bar(position = "fill",stat = "identity")+
  facet_grid(.~year) +
  coord_flip() +
  labs(
        title = "distribution of data_value for responses",
        y = "Average data_value",
        x = "response",
        caption = "Data from p8105.datasets package") +
  theme(legend.position = "bottom")

```



Form the figure above, we could see the same trend of mean of data_value between 2006 and 2010. In response is poor group, the data value has lowest mean. The highest mean of data value appers in "very good" response group.


##problem 3


*Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset*

```{r}
acceler = read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(week_vs_weekend = ifelse(day %in% c("Saturday","Sunday"),"weekend","weekday")) %>% 
   mutate(day = factor(day, levels = c("Monday", "Tuesday","Wednesday", "Thursday", "Friday","Saturday","Sunday"))) %>% 
  select(week, day_id, day, week_vs_weekend, everything())

skimr::skim(acceler)
```


There are 35 rows and 1444 columns in this dataset.
week: unique week ID
day: which day in the week
week_vs_weekend: indicate if the day is in weekday or weekend.
day_id: unique day ID
activity.*: the activity counts for each minute of a 24-hour day starting at midnight

*Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?*

```{r}
acceler %>% 
  group_by(week,day) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes", 
    values_to = "activity") %>% 
  group_by(week,day) %>% 
  summarise(total_activity = sum(activity)) %>% 
  arrange(week,day) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>% 
 knitr::kable()

```



I didn't see any apparent trend through each day in 5 weeks.


*Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.*

```{r}
acceler_plot=acceler %>% 
  group_by(week,day) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes", 
    values_to = "activity")

ggplot(acceler_plot, aes(x = minutes, y = activity,group = day_id,color = day))+
  geom_line()+
  labs(
    title = "24-hour activity time courses for each day and use color to indicate day of the week",
    x = "Minutes",
    y = "Activity"
  )


```


Here is the 24-hour activity time courses for each day and use color to indicate day of the week. From this graph, we can see the activity in Wednesday is apparently lower than the other days. 

